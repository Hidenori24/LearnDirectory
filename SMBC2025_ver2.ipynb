{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmuj60oQp0luA8UOGoBraU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hidenori24/LearnDirectory/blob/master/SMBC2025_ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. ライブラリセットアップ\n"
      ],
      "metadata": {
        "id": "5idmRN9YE8EI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0ZWUiAp_EmOg"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 0. ライブラリ & CFG 定義\n",
        "# ============================================\n",
        "!pip install -q xgboost optuna japanize-matplotlib folium\n",
        "\n",
        "import os, warnings, random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import japanize_matplotlib\n",
        "import folium\n",
        "from xgboost import XGBRegressor\n",
        "import optuna\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------- CFG ----------\n",
        "class CFG:\n",
        "    seed         = 42\n",
        "\n",
        "    data_path    = '/content/drive/MyDrive/ML/Signate_1634/'\n",
        "    corr_threshold = 0.13                 # 相関係数の閾値\n",
        "    comfort_index  = 65                   # 不快指数の快適基準\n",
        "    optuna_trials  = 50                   # Optuna 試行回数\n",
        "\n",
        "\n",
        "# set seed\n",
        "random.seed(CFG.seed)\n",
        "np.random.seed(CFG.seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Google Drive マウント\n"
      ],
      "metadata": {
        "id": "fMaGVka5FB7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 1. Google Drive マウント\n",
        "# ============================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG6UMAE3Eqdv",
        "outputId": "299bf7cb-bb27-40f6-819e-a25252a2fcfe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. データ読み込み"
      ],
      "metadata": {
        "id": "guK8i2niFK0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2. データ読み込み\n",
        "#    - index を DatetimeIndex（UTC）に\n",
        "# =========================================================\n",
        "def read_data(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)                 # まずは普通に読み込み\n",
        "    df['time'] = pd.to_datetime(df['time'], utc=True)   # ①文字列→datetime(UTC)\n",
        "    df['time'] = df['time'].dt.tz_convert(None)         # ②タイムゾーン情報を外す（naive へ）\n",
        "    df = df.set_index('time').sort_index()              # ③DatetimeIndex として設定\n",
        "    return df\n",
        "\n",
        "train_df = read_data(os.path.join(CFG.data_path, 'train.csv'))\n",
        "test_df  = read_data(os.path.join(CFG.data_path, 'test.csv'))\n",
        "\n",
        "# タイムゾーンを UTC→Etc/GMT-1 に変換\n",
        "train_df.index = pd.to_datetime(train_df.index, utc=True).tz_convert('Etc/GMT-1')\n",
        "test_df.index = pd.to_datetime(test_df.index,  utc=True).tz_convert('Etc/GMT-1')\n",
        "\n",
        "print('train', train_df.shape, 'test', test_df.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJKz_c6jEsq1",
        "outputId": "db17362e-e998-400b-8bae-6c35483d81a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train (26280, 91) test (8760, 90)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 前処理"
      ],
      "metadata": {
        "id": "OTed2aDH_i-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import holidays\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import lightgbm as lgb\n",
        "\n",
        "# weather_main, weather_icon, weather_id 列の削除\n",
        "for kw in ['weather_main','weather_icon','weather_id']:\n",
        "    cols = [c for c in train_df.columns if kw in c]\n",
        "    train_df.drop(columns=cols, inplace=True)\n",
        "    test_df.drop(columns=cols, inplace=True)\n",
        "\n",
        "# 値がすべて同じ列の削除\n",
        "constant_cols = [c for c in train_df.columns if train_df[c].nunique() == 1]\n",
        "train_df.drop(columns=constant_cols, inplace=True)\n",
        "test_df.drop(columns=constant_cols, inplace=True)\n",
        "\n",
        "# 大気圧のクリッピング（1090 hPa で上限）\n",
        "pressure_cols = [c for c in train_df.columns if 'pressure' in c]\n",
        "train_df[pressure_cols] = train_df[pressure_cols].clip(upper=1090)\n",
        "test_df [pressure_cols] = test_df [pressure_cols].clip(upper=1090)\n",
        "\n",
        "# 風速のクリッピング（18 m/s で上限）\n",
        "wind_cols = [c for c in train_df.columns if 'wind_speed' in c]\n",
        "train_df[wind_cols] = train_df[wind_cols].clip(upper=18)\n",
        "test_df [wind_cols] = test_df [wind_cols].clip(upper=18)\n",
        "\n",
        "# 欠損値を前方埋め、後方埋め\n",
        "train_df.ffill(inplace=True); train_df.bfill(inplace=True)\n",
        "test_df .ffill(inplace=True); test_df .bfill(inplace=True)"
      ],
      "metadata": {
        "id": "nbHCwFeo7TmI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 不快指数 → 差分絶対値"
      ],
      "metadata": {
        "id": "dTc-JyB1FMV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  大気圧の上下クリップ (下限900hPa, 上限1090hPa)\n",
        "pressure_columns = [col for col in train_df.columns if 'pressure' in col]\n",
        "train_df[pressure_columns] = train_df[pressure_columns].clip(lower=900, upper=1090)\n",
        "test_df [pressure_columns] = test_df [pressure_columns].clip(lower=900, upper=1090)\n",
        "\n",
        "\n",
        "def discomfort_index(temp_k, hum):\n",
        "    # ケルビン→摂氏変換\n",
        "    temp_c = temp_k - 273.15\n",
        "    # temp_c * 0.99 - 14.3 を使った不快指数\n",
        "    return temp_c * 0.81 + (hum / 100) * (temp_c * 0.99 - 14.3) + 46.3\n",
        "\n",
        "for city in ['valencia','madrid','bilbao','barcelona','seville']:\n",
        "    # “生”不快指数\n",
        "    train_df[f'di_{city}']      = discomfort_index(train_df[f'{city}_temp'], train_df[f'{city}_humidity'])\n",
        "    test_df [f'di_{city}']      = discomfort_index(test_df [f'{city}_temp'], test_df [f'{city}_humidity'])\n",
        "    # 快適基準CFG.comfort_indexとの差分絶対値\n",
        "    train_df[f'di_{city}_diff_abs'] = (train_df[f'di_{city}'] - CFG.comfort_index).abs()\n",
        "    test_df [f'di_{city}_diff_abs'] = (test_df [f'di_{city}'] - CFG.comfort_index).abs()\n",
        "\n",
        "# 元の temp, humidity, “生”不快指数列は不要なので削除\n",
        "drop_cols = [\n",
        "    c for c in train_df.columns\n",
        "    if ('_temp' in c) or ('_humidity' in c) or (c.startswith('di_') and not c.endswith('_diff_abs'))\n",
        "]\n",
        "train_df.drop(columns=drop_cols, inplace=True)\n",
        "test_df .drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "# ============================================\n",
        "# 5. generation_sum を追加\n",
        "# ============================================\n",
        "gen_cols = [c for c in train_df.columns if c.startswith('generation')]\n",
        "train_df['generation_sum'] = train_df[gen_cols].sum(axis=1)\n",
        "test_df ['generation_sum'] = test_df [gen_cols].sum(axis=1)\n",
        "\n",
        "# ============================================\n",
        "# 6. 相関係数で特徴量選択（数値列のみで計算）\n",
        "# ============================================\n",
        "# 数値列だけを抽出\n",
        "numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 数値列同士の相関行列を計算し、price_actual との相関を取得\n",
        "corr = train_df[numeric_cols].corr()['price_actual'].abs()\n",
        "\n",
        "# 閾値以上かつ price_actual 自身は除外\n",
        "use_cols = corr[corr > CFG.corr_threshold].index.drop('price_actual').tolist()\n",
        "print(\"選択された特徴量:\", use_cols)\n",
        "\n",
        "# ============================================\n",
        "# 7. 日付情報の追加 & One-Hot\n",
        "# ============================================\n",
        "for df in [train_df, test_df]:\n",
        "    df.index = pd.to_datetime(df.index)  # 念のため\n",
        "    df['month']   = df.index.month       # ←ここを追加！\n",
        "    df['hour']    = df.index.hour\n",
        "    df['weekday'] = df.index.dayofweek\n",
        "    df['day_cat'] = df.index.day.map(lambda d: 0 if d <= 10 else 1 if d <= 20 else 2)\n",
        "# month は one-hot に\n",
        "train_df = pd.get_dummies(train_df, columns=['month'], prefix='month', drop_first=True)\n",
        "test_df  = pd.get_dummies(test_df,  columns=['month'], prefix='month', drop_first=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNTw5dRGEuCe",
        "outputId": "ea64b7e8-3f6b-4d0d-cadc-0f8ce3d99397"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "選択された特徴量: ['generation_biomass', 'generation_fossil_brown_coal/lignite', 'generation_fossil_gas', 'generation_fossil_hard_coal', 'generation_fossil_oil', 'generation_hydro_pumped_storage_consumption', 'generation_hydro_run_of_river_and_poundage', 'total_load_actual', 'valencia_wind_speed', 'madrid_wind_speed', 'bilbao_pressure', 'bilbao_wind_speed', 'bilbao_clouds_all', 'barcelona_pressure', 'barcelona_wind_speed', 'seville_pressure', 'seville_wind_deg', 'di_valencia_diff_abs', 'generation_sum']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.学習／検証データの準備"
      ],
      "metadata": {
        "id": "PfqCv0ijFOM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df[use_cols + ['hour','weekday','day_cat'] + [c for c in train_df.columns if c.startswith('month_')]]\n",
        "y = train_df['price_actual']\n",
        "X_test = test_df[X.columns]  # テストデータの列順を合わせる\n",
        "\n",
        "X_train = X[train_df.index.year < 2017]\n",
        "y_train = y[train_df.index.year < 2017]\n",
        "X_val   = X[train_df.index.year == 2017]\n",
        "y_val   = y[train_df.index.year == 2017]\n",
        "\n",
        "print(\"train:\", X_train.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)"
      ],
      "metadata": {
        "id": "KIQ-glPpEweF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fce61e-a1c3-4151-bbee-adf5dfff3051"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: (17520, 33) val: (8760, 33) test: (8760, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. 年度別分割とアンサンブル学習"
      ],
      "metadata": {
        "id": "Kgev0x-kFPg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 各年度を定義\n",
        "years = sorted(train_df.index.year.unique()) # 例: [2015, 2016, 2017]\n",
        "\n",
        "# 各分割での予測結果を格納するリスト\n",
        "test_predictions = []\n",
        "\n",
        "# 3通りの分割でモデルを学習\n",
        "for held_out_year in years:\n",
        "    print(f\"\\n--- 学習: 検証年度 {held_out_year} ---\")\n",
        "\n",
        "    # 学習データと検証データの分割\n",
        "    train_years = [y for y in years if y != held_out_year]\n",
        "    X_train_fold = X[X.index.year.isin(train_years)]\n",
        "    y_train_fold = y[y.index.year.isin(train_years)]\n",
        "    X_val_fold   = X[X.index.year == held_out_year]\n",
        "    y_val_fold   = y[y.index.year == held_out_year]\n",
        "\n",
        "    print(f\"学習データ ({train_years}):\", X_train_fold.shape, \"検証データ ({held_out_year}):\", X_val_fold.shape)\n",
        "\n",
        "    # Optunaによるパラメータチューニング（各分割ごとに実行）\n",
        "    # 注意: 各分割でOptunaを実行すると計算時間が大幅に増加します。\n",
        "    # 計算時間を抑えたい場合は、一度全体でOptunaを実行したベストパラメータを使用するか、\n",
        "    # Optunaの試行回数を減らすなどを検討してください。\n",
        "    def fold_objective(trial):\n",
        "        params = {\n",
        "            'objective':        'reg:quantileerror',\n",
        "            'eval_metric':      'rmse',\n",
        "            'learning_rate':    trial.suggest_float('learning_rate', 0.001, 0.2),\n",
        "            'n_estimators':     trial.suggest_int('n_estimators', 100, 1000),\n",
        "            'max_depth':        trial.suggest_int('max_depth', 5, 10),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "            'gamma':            trial.suggest_float('gamma', 0.001, 0.1),\n",
        "            'subsample':        trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "            'lambda':           trial.suggest_float('lambda', 0.001, 1.0),\n",
        "            'quantile_alpha':   trial.suggest_float('quantile_alpha', 0.7, 0.9),\n",
        "            'verbosity':        0,\n",
        "            'random_state':     CFG.seed\n",
        "        }\n",
        "        model = XGBRegressor(**params)\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "        preds = model.predict(X_val_fold)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val_fold, preds))\n",
        "        return rmse\n",
        "\n",
        "    # 各分割でOptunaを実行する場合\n",
        "    # study_fold = optuna.create_study(direction='minimize',\n",
        "    #                                 sampler=optuna.samplers.TPESampler(seed=CFG.seed))\n",
        "    # study_fold.optimize(fold_objective, n_trials=CFG.optuna_trials)\n",
        "    # best_params_fold = study_fold.best_params\n",
        "    # print(f\"検証年度 {held_out_year} のベストパラメータ:\", best_params_fold)\n",
        "\n",
        "    # 計算時間削減のため、ここでは全体のOptunaで得られたbest_paramsを使用すると仮定\n",
        "    # もし各分割でOptunaを実行したい場合は、上記のコメントアウトを解除し、\n",
        "    # best_params の代わりに best_params_fold を使用してください。\n",
        "    best_params_fold = best_params # 例: 全体で得られたbest_paramsを使用\n",
        "\n",
        "    # ベストパラメータでモデルを学習（学習データ全体を使用）\n",
        "    model = XGBRegressor(\n",
        "        objective='reg:quantileerror',\n",
        "        eval_metric='rmse',\n",
        "        verbosity=0,\n",
        "        random_state=CFG.seed,\n",
        "        **best_params_fold\n",
        "    )\n",
        "    # 注意: アンサンブルのため、ここでは学習データ全体(X_train_fold, y_train_fold)ではなく、\n",
        "    # 該当の学習期間で学習します。\n",
        "    model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    # テストデータの予測\n",
        "    fold_test_pred = model.predict(X_test)\n",
        "    test_predictions.append(fold_test_pred)\n",
        "    # この分割でのテスト予測結果を submission ファイルとして出力\n",
        "    sub_fold = pd.read_csv(CFG.data_path + 'sample_submit.csv', header=None)\n",
        "    sub_fold[1] = fold_test_pred\n",
        "    output_filename = f'submission_held_out_{held_out_year}.csv'\n",
        "    sub_fold.to_csv(output_filename, header=False, index=False)\n",
        "    print(f\"  >>> {output_filename} を出力しました\")\n",
        "\n",
        "# アンサンブル（ここでは平均をとる）\n",
        "y_test_pred = np.mean(test_predictions, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rJWMv2JEx8O",
        "outputId": "b799d16d-38de-48dc-b2c1-0dc869d45ae0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 学習: 検証年度 2015 ---\n",
            "学習データ ([2016, 2017]): (17544, 33) 検証データ ({held_out_year}): (8736, 33)\n",
            "  >>> submission_held_out_2015.csv を出力しました\n",
            "\n",
            "--- 学習: 検証年度 2016 ---\n",
            "学習データ ([2015, 2017]): (17496, 33) 検証データ ({held_out_year}): (8784, 33)\n",
            "  >>> submission_held_out_2016.csv を出力しました\n",
            "\n",
            "--- 学習: 検証年度 2017 ---\n",
            "学習データ ([2015, 2016]): (17520, 33) 検証データ ({held_out_year}): (8760, 33)\n",
            "  >>> submission_held_out_2017.csv を出力しました\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. ベストパラメータで最終モデル学習 → テスト予測\n"
      ],
      "metadata": {
        "id": "Eb1GfdCO_4tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 欠番\n",
        "# model = XGBRegressor(\n",
        "#     objective='reg:quantileerror',\n",
        "#     eval_metric='rmse',\n",
        "#     verbosity=0,\n",
        "#     random_state=CFG.seed,\n",
        "#     **best_params\n",
        "# )\n",
        "# model.fit(X, y)\n",
        "# y_test_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "zOw68NFY_Ze0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Submission ファイル出力"
      ],
      "metadata": {
        "id": "qgQ4CfGfFQs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub = pd.read_csv(CFG.data_path + 'sample_submit.csv', header=None)\n",
        "sub[1] = y_test_pred\n",
        "sub.to_csv('submission.csv', header=False, index=False)\n",
        "print(\"\\n>>> submission.csv を出力しました\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQYxehpDEz2r",
        "outputId": "b16c9062-b50f-4b17-90d0-2cc73933ee47"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> submission.csv を出力しました\n"
          ]
        }
      ]
    }
  ]
}